# LLMs-Transformers: Transformer BileÅŸenlerini SÄ±fÄ±rdan Uygulama ğŸš€

Bu repo, modern bÃ¼yÃ¼k dil modellerinin (LLM) temelini oluÅŸturan Transformer mimarisinin tÃ¼m ana bileÅŸenlerini **NumPy** ile sÄ±fÄ±rdan, adÄ±m adÄ±m ve modÃ¼ler olarak inÅŸa etmektedir. Her klasÃ¶r, Transformer'Ä±n bir parÃ§asÄ±nÄ± izole ÅŸekilde ele alÄ±r ve matematiksel sezgiyi Ã¶ne Ã§Ä±karÄ±r.

## ğŸ“¦ Proje KlasÃ¶rleri ve Ä°Ã§erikleri

- **embedding-and-vocab/**: Kelime gÃ¶mme (embedding) ve temel kelime daÄŸarcÄ±ÄŸÄ± iÅŸlemleri.
- **tokenizer-basics/**: Byte Pair Encoding (BPE) ile tokenizasyonun temelleri.
- **positional-encoding/**: Sine ve Cosine fonksiyonlarÄ±yla pozisyonel kodlama.
- **micro-attention/**: Scaled Dot-Product Attention'Ä±n saf NumPy ile gÃ¶rselleÅŸtirilmiÅŸ uygulamasÄ±.
- **multi-head-attention/**: Ã‡oklu baÅŸlÄ± dikkat mekanizmasÄ±nÄ±n matematiksel olarak bÃ¶lÃ¼nmÃ¼ÅŸ ve paralel Ã§alÄ±ÅŸan versiyonu.
- **layer-normalization/**: Transformer'larda istikrar iÃ§in Layer Normalization'Ä±n sÄ±fÄ±rdan inÅŸasÄ±.
- **feed-forward-network/**: Her pozisyona baÄŸÄ±msÄ±z uygulanan iki katmanlÄ± doÄŸrusal aÄŸ (FFN).
- **training-loop-basics/**: (BoÅŸ veya temel eÄŸitim dÃ¶ngÃ¼sÃ¼ Ã¶rnekleri iÃ§in ayrÄ±lmÄ±ÅŸtÄ±r.)
- **transformer-encoder-layer/**: TÃ¼m bileÅŸenlerin birleÅŸimiyle tam bir Transformer Encoder BloÄŸu.
- **transformer-output-head/**: Model Ã§Ä±ktÄ±sÄ±nÄ± kelime olasÄ±lÄ±klarÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren Ã§Ä±kÄ±ÅŸ baÅŸÄ±.

## ğŸ› ï¸ Teknolojiler

- **Python 3.x**
- **NumPy**: TÃ¼m matris iÅŸlemleri ve doÄŸrusal cebir iÃ§in
- **Matplotlib**: Dikkat haritalarÄ± ve pozisyonel kodlama gÃ¶rselleÅŸtirmeleri

## ğŸš¦ HÄ±zlÄ± BaÅŸlangÄ±Ã§

Her alt klasÃ¶rde:
```bash
cd klasor-adi
python main.py
```
Ã‡oÄŸu modÃ¼l, Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda Ã¶rnek bir giriÅŸle sonucu veya gÃ¶rselleÅŸtirmeyi ekrana basar.

## ğŸ“š Her ModÃ¼lÃ¼n AmacÄ±

- **Tokenization & Embedding**: Metni sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme.
- **Positional Encoding**: SÄ±ra bilgisini vektÃ¶rlere ekleme.
- **Attention**: Tokenler arasÄ± iliÅŸkileri Ã¶ÄŸrenme ve gÃ¶rselleÅŸtirme.
- **Multi-Head Attention**: FarklÄ± alt uzaylarda paralel dikkat hesaplama.
- **Layer Normalization**: EÄŸitimde istikrar ve hÄ±z.
- **Feed-Forward Network**: Her pozisyona baÄŸÄ±msÄ±z doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼m.
- **Encoder Layer**: TÃ¼m bileÅŸenlerin birleÅŸimiyle tam bir Transformer bloÄŸu.
- **Output Head**: Model Ã§Ä±ktÄ±sÄ±nÄ± kelime olasÄ±lÄ±klarÄ±na Ã§evirme.

## ğŸ¯ Hedef

Bu repo, Transformer mimarisinin temel taÅŸlarÄ±nÄ± derinlemesine anlamak ve uygulamak isteyenler iÃ§in referans niteliÄŸindedir. Her modÃ¼l baÄŸÄ±msÄ±z olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir ve kolayca incelenebilir.

## ğŸ“„ Lisans

MIT

---

Her klasÃ¶rÃ¼n kendi README dosyasÄ±nda daha fazla teknik detay ve kullanÄ±m Ã¶rneÄŸi bulabilirsiniz.